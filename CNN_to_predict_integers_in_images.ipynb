{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN to predict integers in images.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surfaceowl/google.colab_machinelearning/blob/master/CNN_to_predict_integers_in_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "79V-wQfCYB3M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c6c7cd4-d579-4092-c7b9-a46a056746f4"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3vUMV6xAhtCO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# snippet for uploading local files to colab\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JVL6Bxgjhw1k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "bca01b54-0978-44df-d17d-41143b69c779"
      },
      "cell_type": "code",
      "source": [
        "# # colab does not persist local libraries, so we have to reinstall; colab requires prefix of `!`\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install seaborn"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.14.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (2.1.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2018.7)\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.14.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.3.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MtYh_DnHhzV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a5c26981-be49-4753-d087-e5e4f8072cfc"
      },
      "cell_type": "code",
      "source": [
        "# import core python libraries for analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# import graphics\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "%matplotlib inline\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
        "\n",
        "# enable multiple outputs per cell \n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>.container { width:100% !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "c50_zWQIh0D2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import machine learning\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ahPF68FAh3Ad",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "8ee0629b-388d-4ed8-aeb4-27434f132fa8"
      },
      "cell_type": "code",
      "source": [
        "# Import MNIST dataset\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-5c81c5f4c9da>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1-jC4UZRh5NB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training Parameters\n",
        "learning_rate = 0.001\n",
        "num_steps = 500\n",
        "batch_size = 128\n",
        "display_step = 10\n",
        "\n",
        "# Network Parameters\n",
        "num_input = 784 # MNIST data input (img shape: 28*28)\n",
        "num_classes = 10 # MNIST total classes (0-9 digits)\n",
        "dropout = 0.75 # Dropout, probability to keep units\n",
        "\n",
        "# tf Graph input\n",
        "X = tf.placeholder(tf.float32, [None, num_input])\n",
        "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
        "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nZgXRhhYiG_5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create some wrappers \n",
        "def conv2d(x, W, b, strides=1):\n",
        "    # Conv2D wrapper, with bias and relu activation\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def maxpool2d(x, k=2):\n",
        "    # MaxPool2D wrapper\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
        "                          padding='SAME')\n",
        "\n",
        "\n",
        "# Create model\n",
        "def conv_net(x, weights, biases, dropout):\n",
        "    # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
        "    # Reshape to match picture format [Height x Width x Channel]\n",
        "    # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
        "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv1 = maxpool2d(conv1, k=2)\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv2 = maxpool2d(conv2, k=2)\n",
        "\n",
        "    # Fully connected layer\n",
        "    # Reshape conv2 output to fit fully connected layer input\n",
        "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    # Apply Dropout\n",
        "    fc1 = tf.nn.dropout(fc1, dropout)\n",
        "\n",
        "    # Output, class prediction\n",
        "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ZBQJzbkiQ6i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "ddcbe090-5ece-4359-a4a2-9641cd587d1f"
      },
      "cell_type": "code",
      "source": [
        "# Store  weight & bias\n",
        "weights = {\n",
        "    # 5x5 conv, 1 input, 32 outputs\n",
        "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
        "    # 5x5 conv, 32 inputs, 64 outputs\n",
        "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
        "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
        "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
        "    # 1024 inputs, 10 outputs (class prediction)\n",
        "    'out': tf.Variable(tf.random_normal([1024, num_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([32])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
        "}\n",
        "\n",
        "# Construct model\n",
        "logits = conv_net(X, weights, biases, keep_prob)\n",
        "prediction = tf.nn.softmax(logits)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "\n",
        "# Evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-829dda0dfe0a>:25: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SdgBU0H8iVWx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "outputId": "3aeea39a-cbf6-49ec-bbd3-1eaa26d107fa"
      },
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "\n",
        "    for step in range(1, num_steps+1):\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
        "        if step % display_step == 0 or step == 1:\n",
        "            # Calculate batch loss and accuracy\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
        "                                                                 Y: batch_y,\n",
        "                                                                 keep_prob: 1.0})\n",
        "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
        "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(acc))\n",
        "\n",
        "    print(\"\\nOptimization Finished!\")\n",
        "\n",
        "    # Calculate accuracy for 256 MNIST test images\n",
        "    print(\"Testing Accuracy:\", \\\n",
        "        sess.run(accuracy, feed_dict={X: mnist.test.images[:10000],\n",
        "                                      Y: mnist.test.labels[:10000],\n",
        "                                      keep_prob: 1.0}))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1, Minibatch Loss= 53999.2656, Training Accuracy= 0.164\n",
            "Step 10, Minibatch Loss= 20250.0898, Training Accuracy= 0.312\n",
            "Step 20, Minibatch Loss= 9732.8242, Training Accuracy= 0.578\n",
            "Step 30, Minibatch Loss= 6423.3804, Training Accuracy= 0.711\n",
            "Step 40, Minibatch Loss= 3923.4619, Training Accuracy= 0.812\n",
            "Step 50, Minibatch Loss= 5423.8120, Training Accuracy= 0.734\n",
            "Step 60, Minibatch Loss= 4151.4365, Training Accuracy= 0.812\n",
            "Step 70, Minibatch Loss= 3323.5530, Training Accuracy= 0.836\n",
            "Step 80, Minibatch Loss= 3097.9778, Training Accuracy= 0.820\n",
            "Step 90, Minibatch Loss= 3368.9004, Training Accuracy= 0.812\n",
            "Step 100, Minibatch Loss= 3345.8354, Training Accuracy= 0.844\n",
            "Step 110, Minibatch Loss= 2044.6641, Training Accuracy= 0.914\n",
            "Step 120, Minibatch Loss= 2286.5845, Training Accuracy= 0.898\n",
            "Step 130, Minibatch Loss= 3004.8926, Training Accuracy= 0.844\n",
            "Step 140, Minibatch Loss= 1460.6270, Training Accuracy= 0.922\n",
            "Step 150, Minibatch Loss= 1976.3475, Training Accuracy= 0.867\n",
            "Step 160, Minibatch Loss= 315.1777, Training Accuracy= 0.969\n",
            "Step 170, Minibatch Loss= 1974.1036, Training Accuracy= 0.906\n",
            "Step 180, Minibatch Loss= 1291.2589, Training Accuracy= 0.891\n",
            "Step 190, Minibatch Loss= 531.3866, Training Accuracy= 0.938\n",
            "Step 200, Minibatch Loss= 1524.8171, Training Accuracy= 0.906\n",
            "Step 210, Minibatch Loss= 1928.4088, Training Accuracy= 0.883\n",
            "Step 220, Minibatch Loss= 2452.7710, Training Accuracy= 0.883\n",
            "Step 230, Minibatch Loss= 2101.6470, Training Accuracy= 0.930\n",
            "Step 240, Minibatch Loss= 1127.5435, Training Accuracy= 0.945\n",
            "Step 250, Minibatch Loss= 775.4669, Training Accuracy= 0.914\n",
            "Step 260, Minibatch Loss= 614.5030, Training Accuracy= 0.953\n",
            "Step 270, Minibatch Loss= 482.8055, Training Accuracy= 0.969\n",
            "Step 280, Minibatch Loss= 703.5732, Training Accuracy= 0.930\n",
            "Step 290, Minibatch Loss= 1553.6882, Training Accuracy= 0.914\n",
            "Step 300, Minibatch Loss= 1687.1003, Training Accuracy= 0.938\n",
            "Step 310, Minibatch Loss= 1221.4551, Training Accuracy= 0.922\n",
            "Step 320, Minibatch Loss= 441.2582, Training Accuracy= 0.938\n",
            "Step 330, Minibatch Loss= 846.7488, Training Accuracy= 0.945\n",
            "Step 340, Minibatch Loss= 1362.0789, Training Accuracy= 0.938\n",
            "Step 350, Minibatch Loss= 965.7421, Training Accuracy= 0.945\n",
            "Step 360, Minibatch Loss= 1238.8704, Training Accuracy= 0.930\n",
            "Step 370, Minibatch Loss= 1566.4187, Training Accuracy= 0.922\n",
            "Step 380, Minibatch Loss= 903.8369, Training Accuracy= 0.922\n",
            "Step 390, Minibatch Loss= 1179.1360, Training Accuracy= 0.930\n",
            "Step 400, Minibatch Loss= 357.1857, Training Accuracy= 0.938\n",
            "Step 410, Minibatch Loss= 1166.0081, Training Accuracy= 0.930\n",
            "Step 420, Minibatch Loss= 689.8002, Training Accuracy= 0.922\n",
            "Step 430, Minibatch Loss= 574.7736, Training Accuracy= 0.938\n",
            "Step 440, Minibatch Loss= 259.9323, Training Accuracy= 0.961\n",
            "Step 450, Minibatch Loss= 907.3613, Training Accuracy= 0.938\n",
            "Step 460, Minibatch Loss= 324.2157, Training Accuracy= 0.969\n",
            "Step 470, Minibatch Loss= 269.7130, Training Accuracy= 0.977\n",
            "Step 480, Minibatch Loss= 652.6466, Training Accuracy= 0.945\n",
            "Step 490, Minibatch Loss= 258.4712, Training Accuracy= 0.969\n",
            "Step 500, Minibatch Loss= 329.9119, Training Accuracy= 0.969\n",
            "\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.9527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9hjPbosHixYE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}