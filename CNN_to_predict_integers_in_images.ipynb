{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN to predict integers in images.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surfaceowl/google.colab_machinelearning/blob/master/CNN_to_predict_integers_in_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "79V-wQfCYB3M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c6c7cd4-d579-4092-c7b9-a46a056746f4"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3vUMV6xAhtCO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# snippet for uploading local files to colab\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JVL6Bxgjhw1k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "bca01b54-0978-44df-d17d-41143b69c779"
      },
      "cell_type": "code",
      "source": [
        "# # colab does not persist local libraries, so we have to reinstall; colab requires prefix of `!`\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install seaborn"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.14.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (2.1.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2018.7)\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.14.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.3.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MtYh_DnHhzV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a5c26981-be49-4753-d087-e5e4f8072cfc"
      },
      "cell_type": "code",
      "source": [
        "# import core python libraries for analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# import graphics\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "%matplotlib inline\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
        "\n",
        "# enable multiple outputs per cell \n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>.container { width:100% !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "c50_zWQIh0D2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import machine learning\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ahPF68FAh3Ad",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "8ee0629b-388d-4ed8-aeb4-27434f132fa8"
      },
      "cell_type": "code",
      "source": [
        "# Import MNIST dataset\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-5c81c5f4c9da>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1-jC4UZRh5NB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training Parameters\n",
        "learning_rate = 0.001\n",
        "num_steps = 500\n",
        "batch_size = 128\n",
        "display_step = 10\n",
        "\n",
        "# Network Parameters\n",
        "num_input = 784 # MNIST data input (img shape: 28*28)\n",
        "num_classes = 10 # MNIST total classes (0-9 digits)\n",
        "dropout = 0.75 # Dropout, probability to keep units\n",
        "\n",
        "# tf Graph input\n",
        "X = tf.placeholder(tf.float32, [None, num_input])\n",
        "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
        "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nZgXRhhYiG_5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create some wrappers \n",
        "def conv2d(x, W, b, strides=1):\n",
        "    # Conv2D wrapper, with bias and relu activation\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def maxpool2d(x, k=2):\n",
        "    # MaxPool2D wrapper\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
        "                          padding='SAME')\n",
        "\n",
        "\n",
        "# Create model\n",
        "def conv_net(x, weights, biases, dropout):\n",
        "    # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
        "    # Reshape to match picture format [Height x Width x Channel]\n",
        "    # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
        "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv1 = maxpool2d(conv1, k=2)\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv2 = maxpool2d(conv2, k=2)\n",
        "\n",
        "    # Fully connected layer\n",
        "    # Reshape conv2 output to fit fully connected layer input\n",
        "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    # Apply Dropout\n",
        "    fc1 = tf.nn.dropout(fc1, dropout)\n",
        "\n",
        "    # Output, class prediction\n",
        "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ZBQJzbkiQ6i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "ddcbe090-5ece-4359-a4a2-9641cd587d1f"
      },
      "cell_type": "code",
      "source": [
        "# Store  weight & bias\n",
        "weights = {\n",
        "    # 5x5 conv, 1 input, 32 outputs\n",
        "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
        "    # 5x5 conv, 32 inputs, 64 outputs\n",
        "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
        "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
        "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
        "    # 1024 inputs, 10 outputs (class prediction)\n",
        "    'out': tf.Variable(tf.random_normal([1024, num_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([32])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
        "}\n",
        "\n",
        "# Construct model\n",
        "logits = conv_net(X, weights, biases, keep_prob)\n",
        "prediction = tf.nn.softmax(logits)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "\n",
        "# Evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-829dda0dfe0a>:25: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SdgBU0H8iVWx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "outputId": "3a749265-e20a-4478-a7f4-99075a1b14b9"
      },
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "\n",
        "    for step in range(1, num_steps+1):\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
        "        if step % display_step == 0 or step == 1:\n",
        "            # Calculate batch loss and accuracy\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
        "                                                                 Y: batch_y,\n",
        "                                                                 keep_prob: 1.0})\n",
        "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
        "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(acc))\n",
        "\n",
        "    print(\"\\nOptimization Finished!\")\n",
        "\n",
        "    # Calculate accuracy for 256 MNIST test images\n",
        "    print(\"Testing Accuracy:\", \\\n",
        "        sess.run(accuracy, feed_dict={X: mnist.test.images[:256],\n",
        "                                      Y: mnist.test.labels[:256],\n",
        "                                      keep_prob: 1.0}))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1, Minibatch Loss= 50810.5078, Training Accuracy= 0.078\n",
            "Step 10, Minibatch Loss= 19186.0371, Training Accuracy= 0.234\n",
            "Step 20, Minibatch Loss= 7139.5186, Training Accuracy= 0.578\n",
            "Step 30, Minibatch Loss= 4335.8804, Training Accuracy= 0.688\n",
            "Step 40, Minibatch Loss= 5660.5254, Training Accuracy= 0.680\n",
            "Step 50, Minibatch Loss= 3025.9382, Training Accuracy= 0.828\n",
            "Step 60, Minibatch Loss= 2193.7107, Training Accuracy= 0.867\n",
            "Step 70, Minibatch Loss= 1101.6719, Training Accuracy= 0.898\n",
            "Step 80, Minibatch Loss= 2621.0649, Training Accuracy= 0.844\n",
            "Step 90, Minibatch Loss= 3149.4541, Training Accuracy= 0.812\n",
            "Step 100, Minibatch Loss= 2246.8916, Training Accuracy= 0.883\n",
            "Step 110, Minibatch Loss= 1655.2144, Training Accuracy= 0.883\n",
            "Step 120, Minibatch Loss= 1739.4388, Training Accuracy= 0.883\n",
            "Step 130, Minibatch Loss= 1959.7671, Training Accuracy= 0.922\n",
            "Step 140, Minibatch Loss= 1180.0138, Training Accuracy= 0.930\n",
            "Step 150, Minibatch Loss= 664.5569, Training Accuracy= 0.922\n",
            "Step 160, Minibatch Loss= 1511.3138, Training Accuracy= 0.914\n",
            "Step 170, Minibatch Loss= 469.5455, Training Accuracy= 0.922\n",
            "Step 180, Minibatch Loss= 540.9224, Training Accuracy= 0.938\n",
            "Step 190, Minibatch Loss= 1210.6759, Training Accuracy= 0.914\n",
            "Step 200, Minibatch Loss= 1205.6847, Training Accuracy= 0.938\n",
            "Step 210, Minibatch Loss= 1332.1322, Training Accuracy= 0.914\n",
            "Step 220, Minibatch Loss= 969.9111, Training Accuracy= 0.891\n",
            "Step 230, Minibatch Loss= 1206.5984, Training Accuracy= 0.914\n",
            "Step 240, Minibatch Loss= 467.4244, Training Accuracy= 0.953\n",
            "Step 250, Minibatch Loss= 1213.5380, Training Accuracy= 0.930\n",
            "Step 260, Minibatch Loss= 1201.1425, Training Accuracy= 0.898\n",
            "Step 270, Minibatch Loss= 1283.3287, Training Accuracy= 0.914\n",
            "Step 280, Minibatch Loss= 913.8081, Training Accuracy= 0.938\n",
            "Step 290, Minibatch Loss= 631.5271, Training Accuracy= 0.969\n",
            "Step 300, Minibatch Loss= 408.9817, Training Accuracy= 0.961\n",
            "Step 310, Minibatch Loss= 490.5663, Training Accuracy= 0.961\n",
            "Step 320, Minibatch Loss= 1578.0500, Training Accuracy= 0.914\n",
            "Step 330, Minibatch Loss= 301.0035, Training Accuracy= 0.984\n",
            "Step 340, Minibatch Loss= 450.9323, Training Accuracy= 0.961\n",
            "Step 350, Minibatch Loss= 410.6221, Training Accuracy= 0.922\n",
            "Step 360, Minibatch Loss= 743.7745, Training Accuracy= 0.930\n",
            "Step 370, Minibatch Loss= 1565.2651, Training Accuracy= 0.922\n",
            "Step 380, Minibatch Loss= 602.5249, Training Accuracy= 0.945\n",
            "Step 390, Minibatch Loss= 414.3618, Training Accuracy= 0.969\n",
            "Step 400, Minibatch Loss= 493.2334, Training Accuracy= 0.953\n",
            "Step 410, Minibatch Loss= 384.0020, Training Accuracy= 0.984\n",
            "Step 420, Minibatch Loss= 935.9904, Training Accuracy= 0.922\n",
            "Step 430, Minibatch Loss= 612.9078, Training Accuracy= 0.969\n",
            "Step 440, Minibatch Loss= 764.6210, Training Accuracy= 0.953\n",
            "Step 450, Minibatch Loss= 676.6144, Training Accuracy= 0.945\n",
            "Step 460, Minibatch Loss= 1116.8032, Training Accuracy= 0.922\n",
            "Step 470, Minibatch Loss= 553.4305, Training Accuracy= 0.945\n",
            "Step 480, Minibatch Loss= 1023.0123, Training Accuracy= 0.938\n",
            "Step 490, Minibatch Loss= 614.7563, Training Accuracy= 0.945\n",
            "Step 500, Minibatch Loss= 414.3889, Training Accuracy= 0.953\n",
            "\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.98828125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ToYYEPGXiYLC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "outputId": "f4a141cd-ef2a-4765-b573-dd82b9f4b993"
      },
      "cell_type": "code",
      "source": [
        "# Predict single images\n",
        "n_images = 4\n",
        "# Get images from test set\n",
        "test_images = mnist.test.images[:n_images]\n",
        "# Prepare the input data\n",
        "# input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "#    x={'images': test_images}, shuffle=False)\n",
        "\n",
        "# Use the model to predict the images class\n",
        "preds = sess.run(accuracy, feed_dict={X: mnist.test.images[:n_images],\n",
        "                                      Y: mnist.test.labels[:n_images],\n",
        "                                      keep_prob: 1.0})\n",
        "  \n",
        "# Display\n",
        "for i in range(n_images):\n",
        "    plt.imshow(np.reshape(test_images[i], [28, 28]), cmap='gray')\n",
        "    plt.show()\n",
        "    print(\"Model prediction:\", preds[i])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-0e15dc863bab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m preds = sess.run(accuracy, feed_dict={X: mnist.test.images[:n_images],\n\u001b[1;32m     10\u001b[0m                                       \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_images\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                                       keep_prob: 1.0})\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Display\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0;31m# Check session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "9hjPbosHixYE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}