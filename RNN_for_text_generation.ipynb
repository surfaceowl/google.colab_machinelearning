{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN for text generation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surfaceowl/google.colab_machinelearning/blob/master/RNN_for_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "PBKgDEElulB5",
        "colab_type": "code",
        "outputId": "cec3efae-ff58-4ffe-fdda-07b8c8b8eea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/39/53096f9217b057cb049fe872b7fc7ce799a1a89b76cf917d9639e7a558b5/Unidecode-1.0.23-py2.py3-none-any.whl (237kB)\n",
            "\u001b[K    100% |████████████████████████████████| 245kB 4.9MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.0.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T4cfCBGmuwJV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import TensorFlow >= 1.10 and enable eager execution\n",
        "import tensorflow as tf\n",
        "\n",
        "# Note: Once you enable eager execution, it cannot be disabled. \n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import unidecode\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JBtlGWvHuxBE",
        "colab_type": "code",
        "outputId": "0d82e597-ab51-4591-eea8-0c5ecd4d30e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "# alternate file - grimms fairy tails\n",
        "path_to_file = tf.keras.utils.get_file('2591-0.txt', 'http://www.gutenberg.org/files/2591/2591-0.txt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://www.gutenberg.org/files/2591/2591-0.txt\n",
            "565248/560166 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "93RL32tGu2Ar",
        "colab_type": "code",
        "outputId": "4574b401-d74d-4ca3-d0ab-90dcf1cb12d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "text = unidecode.unidecode(open(path_to_file).read())\n",
        "# length of text is the number of characters in it\n",
        "print (len(text))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "540240\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aams6Wwau5PB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# unique contains all the unique characters in the file\n",
        "unique = sorted(set(text))\n",
        "\n",
        "# creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(unique)}\n",
        "idx2char = {i:u for i, u in enumerate(unique)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_oyzzKmJu8BB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# setting the maximum length sentence we want for a single input in characters\n",
        "max_length = 100\n",
        "\n",
        "# length of the vocabulary in chars\n",
        "vocab_size = len(unique)\n",
        "\n",
        "# the embedding dimension \n",
        "embedding_dim = 256\n",
        "\n",
        "# number of RNN (here GRU) units\n",
        "units = 1024\n",
        "\n",
        "# batch size \n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# buffer size to shuffle our dataset\n",
        "BUFFER_SIZE = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J--Bah4mu9Hy",
        "colab_type": "code",
        "outputId": "6d4e463e-e410-44e6-bf60-4c0f9014d466",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "input_text = []\n",
        "target_text = []\n",
        "\n",
        "for f in range(0, len(text)-max_length, max_length):\n",
        "    inps = text[f:f+max_length]\n",
        "    targ = text[f+1:f+1+max_length]\n",
        "\n",
        "    input_text.append([char2idx[i] for i in inps])\n",
        "    target_text.append([char2idx[t] for t in targ])\n",
        "    \n",
        "print (np.array(input_text).shape)\n",
        "print (np.array(target_text).shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5402, 100)\n",
            "(5402, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n8h59EN2vANm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((input_text, target_text)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xNVJd_2cvCgC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, units, batch_size):\n",
        "    super(Model, self).__init__()\n",
        "    self.units = units\n",
        "    self.batch_sz = batch_size\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    if tf.test.is_gpu_available():\n",
        "      self.gru = tf.keras.layers.CuDNNGRU(self.units, \n",
        "                                          return_sequences=True, \n",
        "                                          return_state=True, \n",
        "                                          recurrent_initializer='glorot_uniform')\n",
        "    else:\n",
        "      self.gru = tf.keras.layers.GRU(self.units, \n",
        "                                     return_sequences=True, \n",
        "                                     return_state=True, \n",
        "                                     recurrent_activation='sigmoid', \n",
        "                                     recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # output shape == (batch_size, max_length, hidden_size) \n",
        "    # states shape == (batch_size, hidden_size)\n",
        "\n",
        "    # states variable to preserve the state of the model\n",
        "    # this will be used to pass at every step to the model while training\n",
        "    output, states = self.gru(x, initial_state=hidden)\n",
        "\n",
        "\n",
        "    # reshaping the output so that we can pass it to the Dense layer\n",
        "    # after reshaping the shape is (batch_size * max_length, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # The dense layer will output predictions for every time_steps(max_length)\n",
        "    # output shape after the dense layer == (max_length * batch_size, vocab_size)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fmfZVYzwvF99",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Model(vocab_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XWN6V5YMvIQr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "# using sparse_softmax_cross_entropy so that we don't have to create one-hot vectors\n",
        "def loss_function(real, preds):\n",
        "    return tf.losses.sparse_softmax_cross_entropy(labels=real, logits=preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_OCPt_DkvKRR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 model=model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RETNv4-1vP20",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we will use a custom training loop with the help of GradientTape()\n",
        "\n",
        "We initialize the hidden state of the model with zeros and shape == (batch_size, number of rnn units). We do this by calling the function defined while creating the model.\n",
        "\n",
        "Next, we iterate over the dataset(batch by batch) and calculate the predictions and the hidden states associated with that input.\n",
        "\n",
        "There are a lot of interesting things happening here.\n",
        "\n",
        "The model gets hidden state(initialized with 0), lets call that H0 and the first batch of input, lets call that I0.\n",
        "The model then returns the predictions P1 and H1.\n",
        "For the next batch of input, the model receives I1 and H1.\n",
        "The interesting thing here is that we pass H1 to the model with I1 which is how the model learns. The context learned from batch to batch is contained in the hidden state.\n",
        "We continue doing this until the dataset is exhausted and then we start a new epoch and repeat this.\n",
        "After calculating the predictions, we calculate the loss using the loss function defined above. Then we calculate the gradients of the loss with respect to the model variables(input)\n",
        "\n",
        "Finally, we take a step in that direction with the help of the optimizer using the apply_gradients function.\n",
        "\n",
        "Note:- If you are running this notebook in Colab which has a Tesla K80 GPU it takes about 23 seconds per epoch."
      ]
    },
    {
      "metadata": {
        "id": "3yISwGOVvTP1",
        "colab_type": "code",
        "outputId": "a521c362-f149-4244-9567-32b09b17e4de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1417
        }
      },
      "cell_type": "code",
      "source": [
        "# Training step\n",
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    # initializing the hidden state at the start of every epoch\n",
        "    hidden = model.reset_states()\n",
        "    \n",
        "    for (batch, (inp, target)) in enumerate(dataset):\n",
        "          with tf.GradientTape() as tape:\n",
        "              # feeding the hidden state back into the model\n",
        "              # This is the interesting step\n",
        "              predictions, hidden = model(inp, hidden)\n",
        "              \n",
        "              # reshaping the target because that's how the \n",
        "              # loss function expects it\n",
        "              target = tf.reshape(target, (-1,))\n",
        "              loss = loss_function(target, predictions)\n",
        "              \n",
        "          grads = tape.gradient(loss, model.variables)\n",
        "          optimizer.apply_gradients(zip(grads, model.variables))\n",
        "\n",
        "          if batch % 100 == 0:\n",
        "              print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1,\n",
        "                                                            batch,\n",
        "                                                            loss))\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.4301\n",
            "Epoch 1 Loss 2.3504\n",
            "Time taken for 1 epoch 13.794904708862305 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.3696\n",
            "Epoch 2 Loss 2.0150\n",
            "Time taken for 1 epoch 11.403647899627686 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 2.0679\n",
            "Epoch 3 Loss 1.8547\n",
            "Time taken for 1 epoch 11.41964054107666 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.8692\n",
            "Epoch 4 Loss 1.6768\n",
            "Time taken for 1 epoch 11.44180679321289 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.6367\n",
            "Epoch 5 Loss 1.4849\n",
            "Time taken for 1 epoch 11.583993911743164 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.5318\n",
            "Epoch 6 Loss 1.4849\n",
            "Time taken for 1 epoch 11.481489896774292 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.4408\n",
            "Epoch 7 Loss 1.3584\n",
            "Time taken for 1 epoch 11.455058336257935 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.3290\n",
            "Epoch 8 Loss 1.3010\n",
            "Time taken for 1 epoch 11.458365201950073 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.2572\n",
            "Epoch 9 Loss 1.2576\n",
            "Time taken for 1 epoch 11.467390298843384 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.2363\n",
            "Epoch 10 Loss 1.1804\n",
            "Time taken for 1 epoch 11.533094644546509 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 1.2028\n",
            "Epoch 11 Loss 1.1916\n",
            "Time taken for 1 epoch 11.573818922042847 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.1340\n",
            "Epoch 12 Loss 1.1595\n",
            "Time taken for 1 epoch 11.502313137054443 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.0762\n",
            "Epoch 13 Loss 1.1569\n",
            "Time taken for 1 epoch 11.453646659851074 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.0131\n",
            "Epoch 14 Loss 1.0945\n",
            "Time taken for 1 epoch 11.495882749557495 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.9989\n",
            "Epoch 15 Loss 1.0067\n",
            "Time taken for 1 epoch 11.575844287872314 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.9815\n",
            "Epoch 16 Loss 1.0072\n",
            "Time taken for 1 epoch 11.513092279434204 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.9206\n",
            "Epoch 17 Loss 0.9558\n",
            "Time taken for 1 epoch 11.496836185455322 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.8603\n",
            "Epoch 18 Loss 0.9304\n",
            "Time taken for 1 epoch 11.511039733886719 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.8386\n",
            "Epoch 19 Loss 0.8976\n",
            "Time taken for 1 epoch 11.47329330444336 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.8057\n",
            "Epoch 20 Loss 0.8903\n",
            "Time taken for 1 epoch 11.555030345916748 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p2Ie96F9vXf_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8dda186d-568b-424f-9b5b-0e3b4990c994"
      },
      "cell_type": "code",
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7ff05c988978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "5BJAD5ubveFE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The below code block is used to generated the text\n",
        "\n",
        "We start by choosing a start string and initializing the hidden state and setting the number of characters we want to generate.\n",
        "\n",
        "We get predictions using the start_string and the hidden state\n",
        "\n",
        "Then we use argmax to calculate the index of the predicted word. We use this predicted word as our next input to the model\n",
        "\n",
        "The hidden state returned by the model is fed back into the model so that it now has more context rather than just one word. After we predict the next word, the modified hidden states are again fed back into the model, which is how it learns as it gets more context from the previously predicted words.\n",
        "\n",
        "If you see the predictions, the model knows when to capitalize, make paragraphs and the text follows a shakespeare style of writing which is pretty awesome!"
      ]
    },
    {
      "metadata": {
        "id": "8f4fbEbUveuT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "08731391-611b-4aca-9e69-2b4c6b7fae18"
      },
      "cell_type": "code",
      "source": [
        "# Evaluation step(generating text using the model learned)\n",
        "\n",
        "# number of characters to generate\n",
        "num_generate = 200\n",
        "\n",
        "# You can change the start string to experiment\n",
        "start_string = 'Wolf'\n",
        "# converting our start string to numbers(vectorizing!) \n",
        "input_eval = [char2idx[s] for s in start_string]\n",
        "input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "# empty string to store our results\n",
        "text_generated = ''\n",
        "\n",
        "# hidden state shape == (batch_size, number of rnn units); here batch size == 1\n",
        "hidden = [tf.zeros((1, units))]\n",
        "for i in range(num_generate):\n",
        "    predictions, hidden = model(input_eval, hidden)\n",
        "\n",
        "    # using argmax to predict the word returned by the model\n",
        "    predicted_id = tf.argmax(predictions[-1]).numpy()\n",
        "    \n",
        "    # We pass the predicted word as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    \n",
        "    text_generated += idx2char[predicted_id]\n",
        "\n",
        "print (start_string + text_generated)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wolf!' said the soldier; 'but I will not be a fine thing for your dearest child, and she said to him: 'You may come out of the window and said: 'I have been drinking a side pate at the spindle of the room\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}